{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "164fc8b3-5aa3-4d14-93f0-bdcd5bf2742d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f85e9125-7e1b-4332-9973-ba7fb194d71d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.Monthly Percentage Difference \n",
    "\n",
    "Q. Given a table of purchases by date, calculate the month-over-month percentage change in revenue. The output should include the year-month date (YYYY-MM) and percentage change, rounded to the 2nd decimal point, and sorted from the beginning of the year to the end of the year.\n",
    "The percentage change column will be populated from the 2nd month forward and can be calculated as ((this month's revenue - last month's revenue) / last month's revenue)*100.    \n",
    "[link](https://platform.stratascratch.com/coding/10319-monthly-percentage-difference?code_type=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc58dd43-8c22-4c5c-880c-86f420e2f553",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark-Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3509ba02-0842-47b9-b950-bc39ea442bfb",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+-----------+\n| id|created_at| value|purchase_id|\n+---+----------+------+-----------+\n|  1|2019-01-01|172692|         43|\n|  2|2019-01-05|177194|         36|\n|  3|2019-01-09|109513|         30|\n|  4|2019-01-13|164911|         30|\n|  5|2019-01-17|198872|         39|\n+---+----------+------+-----------+\nonly showing top 5 rows\n\nroot\n |-- id: integer (nullable = true)\n |-- created_at: date (nullable = true)\n |-- value: integer (nullable = true)\n |-- purchase_id: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "sf_transactions = spark.read.format('csv')\\\n",
    "                        .option('header','true')\\\n",
    "                        .option('inferschema','true')\\\n",
    "                        .option('mode','PERMISSIVE')\\\n",
    "                        .load('/FileStore/tables/one.csv')\n",
    "\n",
    "sf_transactions.show(5)\n",
    "sf_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a39759-a880-4cc1-9a61-5739765e034e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n|     ym|revenue_diff_pct|\n+-------+----------------+\n|2019-01|            null|\n|2019-02|          -28.56|\n|2019-03|           23.35|\n|2019-04|          -13.84|\n|2019-05|           13.49|\n|2019-06|           -2.78|\n|2019-07|            -6.0|\n|2019-08|           28.36|\n|2019-09|           -4.97|\n|2019-10|          -12.68|\n|2019-11|            1.71|\n|2019-12|           -2.11|\n+-------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "result_df = sf_transactions.select(\n",
    "            date_format(col(\"created_at\"), \"yyyy-MM\").alias('ym'),col('value'))\\\n",
    "            .groupBy('ym').agg(sum('value').alias('sum(value)'))\\\n",
    "            .withColumn('pre', lag('sum(value)', 1).over(Window.orderBy('ym')))\\\n",
    "            .withColumn('revenue_diff_pct',round(((col('sum(value)')-col('pre'))/col('pre'))*100,2))\\\n",
    "            \n",
    "result_df.select('ym','revenue_diff_pct').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63b69645-a4d8-4741-ab23-acac864e43c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark-SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6febba36-e112-4ff4-9f1e-b7a873c00e83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sf_transactions.createOrReplaceTempView('sf_transactions_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd35d5a6-7a7c-45f6-ae0c-8b0509d2b072",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n|     ym|revenue_diff_pct|\n+-------+----------------+\n|2019-01|            null|\n|2019-02|          -28.56|\n|2019-03|           23.35|\n|2019-04|          -13.84|\n|2019-05|           13.49|\n|2019-06|           -2.78|\n|2019-07|            -6.0|\n|2019-08|           28.36|\n|2019-09|           -4.97|\n|2019-10|          -12.68|\n|2019-11|            1.71|\n|2019-12|           -2.11|\n+-------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select ym,round(((value-pre)/pre)*100,2) as revenue_diff_pct\n",
    "from \n",
    "    (select * , lag(T.value) over(order by ym) as pre\n",
    "    from \n",
    "        (select \n",
    "        DATE_FORMAT(created_at,\"yyyy-MM\") as ym,\n",
    "        sum(value) as value\n",
    "        from \n",
    "        sf_transactions_table\n",
    "        group by ym \n",
    "        ) AS T\n",
    "    ) as U\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17652d2f-4b02-4382-83c3-203a4227befd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Top Percentile Fraud\n",
    "\n",
    "Q. ABC Corp is a mid-sized insurer in the US and in the recent past their fraudulent claims have increased significantly for their personal auto insurance portfolio. They have developed a ML based predictive model to identify propensity of fraudulent claims. Now, they assign highly experienced claim adjusters for top 5 percentile of claims identified by the model.\n",
    "Your objective is to identify the top 5 percentile of claims from each state. Your output should be policy number, state, claim cost, and fraud score.  \n",
    "[link](https://platform.stratascratch.com/coding/10303-top-percentile-fraud?code_type=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7373c189-daab-4dca-837f-3fc0209c6b16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark-Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0a07523-3f14-45cd-8ba2-3b04e05adc27",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-----------+\n|policy_num|state|claim_cost|fraud_score|\n+----------+-----+----------+-----------+\n|  ABCD1001|   CA|      4113|      0.613|\n|  ABCD1002|   CA|      3946|      0.156|\n|  ABCD1003|   CA|      4335|      0.014|\n|  ABCD1004|   CA|      3967|      0.142|\n|  ABCD1005|   CA|      1599|      0.889|\n+----------+-----+----------+-----------+\nonly showing top 5 rows\n\nroot\n |-- policy_num: string (nullable = true)\n |-- state: string (nullable = true)\n |-- claim_cost: integer (nullable = true)\n |-- fraud_score: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "fraud_score_df = spark.read.format('csv')\\\n",
    "                        .option('header','true')\\\n",
    "                        .option('inferschema','true')\\\n",
    "                        .option('mode','PERMISSIVE')\\\n",
    "                        .load('/FileStore/tables/two.csv')\n",
    "\n",
    "fraud_score_df.show(5)\n",
    "fraud_score_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55cf4cf8-ad8f-40cd-9c4e-18a9c962946f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-----------+\n|policy_num|state|claim_cost|fraud_score|\n+----------+-----+----------+-----------+\n|  ABCD1027|   CA|      2663|      0.988|\n|  ABCD1016|   CA|      1639|      0.964|\n|  ABCD1079|   CA|      4224|      0.963|\n|  ABCD1081|   CA|      1080|      0.951|\n|  ABCD1069|   CA|      1426|      0.948|\n|  ABCD1222|   FL|      2392|      0.988|\n|  ABCD1218|   FL|      1419|      0.961|\n|  ABCD1291|   FL|      2581|      0.939|\n|  ABCD1230|   FL|      2560|      0.923|\n|  ABCD1277|   FL|      2057|      0.923|\n|  ABCD1189|   NY|      3577|      0.982|\n|  ABCD1117|   NY|      4903|      0.978|\n|  ABCD1187|   NY|      3722|      0.976|\n|  ABCD1196|   NY|      2994|      0.973|\n|  ABCD1121|   NY|      4009|      0.969|\n|  ABCD1361|   TX|      4950|      0.999|\n|  ABCD1304|   TX|      1407|      0.996|\n|  ABCD1398|   TX|      3191|      0.978|\n|  ABCD1366|   TX|      2453|      0.968|\n|  ABCD1386|   TX|      4311|      0.963|\n+----------+-----+----------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "fraud_score_df.withColumn('rank',rank().over(Window.partitionBy(col('state'))\\\n",
    "                .orderBy(col('fraud_score').desc())))\\\n",
    "                .withColumn('rnk_filter',ceil(max(col('rank')).over(Window.partitionBy(col('state')))*0.05))\\\n",
    "                .filter(col('rank')<=col('rnk_filter'))\\\n",
    "                .select('policy_num','state','claim_cost','fraud_score').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58f41d25-f3f5-4ff6-8307-a4e9e9e5300b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark_SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cce9bb91-1485-45ae-82ad-402841696ea7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fraud_score_df.createOrReplaceTempView('fraud_score_Table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6c4330b-b9c3-44be-b704-3442eb5175fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-----------+\n|policy_num|state|claim_cost|fraud_score|\n+----------+-----+----------+-----------+\n|  ABCD1027|   CA|      2663|      0.988|\n|  ABCD1016|   CA|      1639|      0.964|\n|  ABCD1079|   CA|      4224|      0.963|\n|  ABCD1081|   CA|      1080|      0.951|\n|  ABCD1069|   CA|      1426|      0.948|\n|  ABCD1222|   FL|      2392|      0.988|\n|  ABCD1218|   FL|      1419|      0.961|\n|  ABCD1291|   FL|      2581|      0.939|\n|  ABCD1230|   FL|      2560|      0.923|\n|  ABCD1277|   FL|      2057|      0.923|\n|  ABCD1189|   NY|      3577|      0.982|\n|  ABCD1117|   NY|      4903|      0.978|\n|  ABCD1187|   NY|      3722|      0.976|\n|  ABCD1196|   NY|      2994|      0.973|\n|  ABCD1121|   NY|      4009|      0.969|\n|  ABCD1361|   TX|      4950|      0.999|\n|  ABCD1304|   TX|      1407|      0.996|\n|  ABCD1398|   TX|      3191|      0.978|\n|  ABCD1366|   TX|      2453|      0.968|\n|  ABCD1386|   TX|      4311|      0.963|\n+----------+-----+----------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        select \n",
    "        policy_num,\n",
    "        state,\n",
    "        claim_cost,\n",
    "        fraud_score\n",
    "        from (  select * ,\n",
    "            ceil(((count(*) over(partition by state))*0.05)) as select_rnk ,\n",
    "            rank() over(partition by state order by fraud_score desc) as rnk\n",
    "            from fraud_score_Table\n",
    "            ) as T\n",
    "        where rnk <= select_rnk \n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e7dc12e-b640-48a0-91bb-21aa17243360",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Premium vs Freemium\n",
    "\n",
    "Q. Find the total number of downloads for paying and non-paying users by date. Include only records where non-paying customers have more downloads than paying customers. The output should be sorted by earliest date first and contain 3 columns date, non-paying downloads, paying downloads.    \n",
    "[link](https://platform.stratascratch.com/coding/10300-premium-vs-freemium?code_type=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fdf6c1e-4677-438b-a5e4-a454e26dfd2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark-Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "301c1ecc-9c09-4625-8461-b6baf35474cd",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n|user_id|acc_id|\n+-------+------+\n|      1|   716|\n|      2|   749|\n|      3|   713|\n|      4|   744|\n|      5|   726|\n+-------+------+\nonly showing top 5 rows\n\nroot\n |-- user_id: integer (nullable = true)\n |-- acc_id: integer (nullable = true)\n\n+------+---------------+\n|acc_id|paying_customer|\n+------+---------------+\n|   700|             no|\n|   701|             no|\n|   702|             no|\n|   703|             no|\n|   704|             no|\n+------+---------------+\nonly showing top 5 rows\n\nroot\n |-- acc_id: integer (nullable = true)\n |-- paying_customer: string (nullable = true)\n\n+----------+-------+---------+\n|      date|user_id|downloads|\n+----------+-------+---------+\n|2020-08-24|      1|        6|\n|2020-08-22|      2|        6|\n|2020-08-18|      3|        2|\n|2020-08-24|      4|        4|\n|2020-08-19|      5|        7|\n+----------+-------+---------+\nonly showing top 5 rows\n\nroot\n |-- date: date (nullable = true)\n |-- user_id: integer (nullable = true)\n |-- downloads: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "ms_user_dimension_df = spark.read.format('csv')\\\n",
    "                        .option('header','true')\\\n",
    "                        .option('inferschema','true')\\\n",
    "                        .option('mode','PERMISSIVE')\\\n",
    "                        .load('/FileStore/tables/3_one.csv')\n",
    "\n",
    "ms_acc_dimension_df =   spark.read.format('csv')\\\n",
    "                        .option('header','true')\\\n",
    "                        .option('inferschema','true')\\\n",
    "                        .option('mode','PERMISSIVE')\\\n",
    "                        .load('/FileStore/tables/3_two.csv')\n",
    "\n",
    "ms_download_facts =     spark.read.format('csv')\\\n",
    "                        .option('header','true')\\\n",
    "                        .option('inferschema','true')\\\n",
    "                        .option('mode','PERMISSIVE')\\\n",
    "                        .load('/FileStore/tables/3_three.csv')\n",
    "\n",
    "ms_user_dimension_df.show(5)\n",
    "ms_user_dimension_df.printSchema()\n",
    "\n",
    "ms_acc_dimension_df.show(5)\n",
    "ms_acc_dimension_df.printSchema()\n",
    "\n",
    "ms_download_facts.show(5)\n",
    "ms_download_facts.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8fd8bfc-bcfa-44b8-8498-a6ad9ac6d8a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+\n|      date|non_paying|paying|\n+----------+----------+------+\n|2020-08-16|        15|    14|\n|2020-08-17|        45|     9|\n|2020-08-18|        10|     7|\n|2020-08-21|        32|    17|\n+----------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "ms_download_facts.join(ms_user_dimension_df ,ms_download_facts.user_id == ms_user_dimension_df.user_id, 'left' )\\\n",
    "        .join(ms_acc_dimension_df , ms_user_dimension_df.acc_id == ms_acc_dimension_df.acc_id, 'left')\\\n",
    "        .select('date',ms_download_facts.user_id,'downloads',ms_user_dimension_df.acc_id,'paying_customer')\\\n",
    "        .orderBy(col('date').asc())\\\n",
    "        .withColumn('non_paying',sum('downloads').over(Window.partitionBy('date','paying_customer')))\\\n",
    "        .select('date','paying_customer','non_paying').distinct()\\\n",
    "        .withColumn('paying',lead('non_paying').over(Window.partitionBy('date').orderBy('date')))\\\n",
    "        .filter(col('paying').isNotNull() ).select('date','non_paying','paying')\\\n",
    "        .filter(col('non_paying')>col('paying')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee037d6d-85f2-4c14-86de-9aa29b323f6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark_SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "292de340-604b-4975-b4ae-a53f8e2984ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ms_user_dimension_df.createOrReplaceTempView('ms_user_dimension_table')\n",
    "ms_acc_dimension_df.createOrReplaceTempView('ms_acc_dimension_table')\n",
    "ms_download_facts.createOrReplaceTempView('ms_download_facts_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "447fb482-ddb3-4c6d-988c-c8b1c8f69d65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+\n|      date|non_paying|paying|\n+----------+----------+------+\n|2020-08-16|        15|    14|\n|2020-08-17|        45|     9|\n|2020-08-18|        10|     7|\n|2020-08-21|        32|    17|\n+----------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select *\n",
    "from\n",
    "    (select date,\n",
    "    non_paying,\n",
    "    lead(non_paying) over(partition by date order by paying_customer) as paying\n",
    "    from\n",
    "        (select distinct date , \n",
    "        sum(downloads) over(partition by date,paying_customer) as non_paying, \n",
    "        paying_customer \n",
    "        from ms_download_facts_table as a\n",
    "        left join ms_user_dimension_table as b\n",
    "        on a.user_id = b.user_id\n",
    "        left join ms_acc_dimension_table as c\n",
    "        on b.acc_id = c.acc_id\n",
    "        ) as T\n",
    "    ) as U\n",
    "where paying is not null\n",
    "and non_paying > paying ;\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dad4b04-b7c7-4594-966b-6128d4c414c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4. Popularity Percentage\n",
    "\n",
    "Q. Find the popularity percentage for each user on Meta/Facebook. The popularity percentage is defined as the total number of friends the user has divided by the total number of users on the platform, then converted into a percentage by multiplying by 100.\n",
    "Output each user along with their popularity percentage. Order records in ascending order by user id.\n",
    "The 'user1' and 'user2' column are pairs of friends.    \n",
    "[link](https://platform.stratascratch.com/coding/10284-popularity-percentage?code_type=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "587ea226-dd03-4a65-b0f4-bf8c618eb207",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark-Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827974a8-1ce5-45a6-828c-f4fc07234041",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n|user1|user2|\n+-----+-----+\n|    2|    1|\n|    1|    3|\n|    4|    1|\n|    1|    5|\n|    1|    6|\n+-----+-----+\nonly showing top 5 rows\n\nroot\n |-- user1: integer (nullable = true)\n |-- user2: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "my_data = ([(2,1),\n",
    "            (1,3),\n",
    "            (4,1),\n",
    "            (1,5),\n",
    "            (1,6),\n",
    "            (2,6),\n",
    "            (7,2),\n",
    "            (8,3),\n",
    "            (3,9),\n",
    "          ])\n",
    "\n",
    "my_schema = StructType([\n",
    "    StructField('user1',IntegerType()),\n",
    "    StructField('user2',IntegerType())\n",
    "                      ])\n",
    "\n",
    "facebook_friends_df = spark.createDataFrame(my_data,my_schema)\n",
    "\n",
    "facebook_friends_df.show(5)\n",
    "facebook_friends_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78ed5d23-1110-4a98-a0ca-b7240b7ccccd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n|user1|popularity_percent|\n+-----+------------------+\n|    1|            55.556|\n|    2|            33.333|\n|    3|            33.333|\n|    4|            11.111|\n|    5|            11.111|\n|    6|            22.222|\n|    7|            11.111|\n|    8|            11.111|\n|    9|            11.111|\n+-----+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "facebook_friends_df.select('user1').union(facebook_friends_df.select('user2').alias('user1'))\\\n",
    "                .withColumn('user_pop',count('user1').over(Window.partitionBy('user1'))) \\\n",
    "                .withColumn('total_user', lit(result.select(col('user1')).distinct().count()))\\\n",
    "                .distinct()\\\n",
    "                .withColumn('popularity_percent',round(col('user_pop')/col('total_user')*100,3))\\\n",
    "                .drop(col('user_pop'),col('total_user')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31055d36-5b80-4575-9dda-045bd9e37a04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark_SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cf77cb3-65ec-44dc-8d06-1d13ff1500d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "facebook_friends_df.createOrReplaceTempView('facebook_friends_df_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3388e39e-b911-441a-b519-5a6c0d2b8e05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n|user1|popularity_percent|\n+-----+------------------+\n|    1|            55.556|\n|    2|            33.333|\n|    3|            33.333|\n|    4|            11.111|\n|    5|            11.111|\n|    6|            22.222|\n|    7|            11.111|\n|    8|            11.111|\n|    9|            11.111|\n+-----+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select user1, round((pop_count/total_count)*100,3) as popularity_percent\n",
    "from\n",
    "    (select  distinct user1 , \n",
    "     count(user1) over(partition by user1)  as pop_count ,\n",
    "     (count(user1) over())/2 as total_count\n",
    "     from\n",
    "        (SELECT user1, user2 \n",
    "        FROM facebook_friends_df_table \n",
    "        UNION \n",
    "        SELECT user2 AS user1, user1 AS user2 \n",
    "        FROM facebook_friends_df_table\n",
    "        ) as T\n",
    "    ) as U\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbae52c0-45a8-4fa8-8a66-8efbc5a6f4ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5. Top 5 States With 5 Star Businesses\n",
    "\n",
    "Q. Find the top 5 states with the most 5 star businesses. Output the state name along with the number of 5-star businesses and order records by the number of 5-star businesses in descending order. In case there are ties in the number of businesses, return all the unique states. If two states have the same result, sort them in alphabetical order.     \n",
    "[Link](https://platform.stratascratch.com/coding/10046-top-5-states-with-5-star-businesses?code_type=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "486034a3-7466-4f93-a11e-f9c65d8e0ace",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark-Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37119892-1f75-497e-a3c7-7902a50c07d6",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+--------------------+--------------+-----+-----------+--------+---------+-----+------------+-------+--------------------+\n|         business_id|                name|neighborhood|             address|          city|state|postal_code|latitude|longitude|stars|review_count|is_open|          categories|\n+--------------------+--------------------+------------+--------------------+--------------+-----+-----------+--------+---------+-----+------------+-------+--------------------+\n|G5ERFWvPfHy7IDAUY...|All Colors Mobile...|        null|     7137 N 28th Ave|       Phoenix|   AZ|      85051|  33.448| -112.074|  1.0|           4|      1|Auto Detailing;Au...|\n|0jDvRJS-z9zdMgOUX...|             Sunfare|        null|811 W Deer Valley Rd|       Phoenix|   AZ|      85027|  33.683| -112.085|  5.0|          27|      1|Personal Chefs;Fo...|\n|6HmDqeNNZtHMK0t2g...|     Dry Clean Vegas|   Southeast|2550 Windmill Ln,...|     Las Vegas|   NV|      89123|  36.042| -115.118|  1.0|           4|      1|Dry Cleaning & La...|\n|pbt3SBcEmxCfZPdnm...|   The Cuyahoga Room|        null|740 Munroe Falls Ave|Cuyahoga Falls|   OH|      44221|   41.14|  -81.472|  1.0|           3|      0|Wedding Planning;...|\n|CX8pfLn7Bk9o2-8yD...|       The UPS Store|        null|4815 E Carefree H...|    Cave Creek|   AZ|      85331|  33.798| -111.977|  1.5|           5|      1|Notaries;Printing...|\n+--------------------+--------------------+------------+--------------------+--------------+-----+-----------+--------+---------+-----+------------+-------+--------------------+\nonly showing top 5 rows\n\nroot\n |-- business_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- neighborhood: string (nullable = true)\n |-- address: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- postal_code: string (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- stars: double (nullable = true)\n |-- review_count: integer (nullable = true)\n |-- is_open: integer (nullable = true)\n |-- categories: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "yelp_business_df = spark.read.format('csv')\\\n",
    "                   .option('header','true')\\\n",
    "                   .option('inferschema','true')\\\n",
    "                   .load('/FileStore/tables/five-1.csv')\n",
    "\n",
    "yelp_business_df.show(5)\n",
    "yelp_business_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65c2c8e4-4eb3-4fac-9052-84ca78ffe682",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+\n|state|five_star_counts|\n+-----+----------------+\n|   AZ|              10|\n|   ON|               5|\n|   NV|               4|\n|   IL|               3|\n|   OH|               3|\n|   WI|               3|\n+-----+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "yelp_business_df.select('state').filter(col('stars')==5)\\\n",
    "        .groupBy('state').agg(count('state').alias('five_star_counts')).orderBy(col('five_star_counts').desc())\\\n",
    "        .withColumn('rnk',rank().over(Window.orderBy(col('five_star_counts').desc())))\\\n",
    "        .orderBy('rnk','state').filter(col('rnk')<=5).drop(col('rnk')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f79b02fe-4343-4a3b-b1d6-2083b409614d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark_SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28405d77-1982-47ad-b309-d45da7255500",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yelp_business_df.createOrReplaceTempView('yelp_business_df_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c854cfa-b7e5-485b-b4fc-3553958aac5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+\n|state|five_star_counts|\n+-----+----------------+\n|   AZ|              10|\n|   ON|               5|\n|   NV|               4|\n|   IL|               3|\n|   OH|               3|\n|   WI|               3|\n+-----+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select state,five_star_counts\n",
    "from\n",
    "    (select *,rank() over(order by five_star_counts desc) as rnk\n",
    "    from \n",
    "        (select state , count(state) as five_star_counts \n",
    "        from yelp_business_df_table\n",
    "        where stars = 5\n",
    "        group by state\n",
    "        order by five_star_counts desc , state \n",
    "        ) as T \n",
    "    ) as U\n",
    "where rnk <= 5\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f49fcd7-2e73-4d12-a59c-b1ffc9343c64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6. Host Popularity Rental Prices\n",
    "\n",
    "Q. You’re given a table of rental property searches by users. The table consists of search results and outputs host information for searchers. Find the minimum, average, maximum rental prices for each host’s popularity rating. The host’s popularity rating is defined as below:  \n",
    "0 reviews: New  \n",
    "1 to 5 reviews: Rising  \n",
    "6 to 15 reviews: Trending Up  \n",
    "16 to 40 reviews: Popular  \n",
    "more than 40 reviews: Hot \n",
    "\n",
    "\n",
    "Tip: The id column in the table refers to the search ID. You'll need to create your own host_id by concating price, room_type, host_since, zipcode, and number_of_reviews.\n",
    "\n",
    "\n",
    "Output host popularity rating and their minimum, average and maximum rental prices.    \n",
    "[Link](https://platform.stratascratch.com/coding/9632-host-popularity-rental-prices?code_type=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61ab12da-802e-46fe-8cee-6e7317f02eef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark-Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "219ad88a-d21c-4825-b37a-89d1639c23b1",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------------+---------------+------------+---------+--------+-------------------+------------+----+----------------------+------------------+----------+-----------------+-----------------+--------------------+-------+--------+----+\n|      id| price|property_type|      room_type|accommodates|bathrooms|bed_type|cancellation_policy|cleaning_fee|city|host_identity_verified|host_response_rate|host_since|    neighbourhood|number_of_reviews|review_scores_rating|zipcode|bedrooms|beds|\n+--------+------+-------------+---------------+------------+---------+--------+-------------------+------------+----+----------------------+------------------+----------+-----------------+-----------------+--------------------+-------+--------+----+\n| 8284881|621.46|        House|Entire home/apt|           8|        3|Real Bed|             strict|           1|  LA|                     f|              100%|2016-11-01|Pacific Palisades|                1|                null|  90272|       4|   6|\n| 8284882|621.46|        House|Entire home/apt|           8|        3|Real Bed|             strict|           1|  LA|                     f|              100%|2016-11-01|Pacific Palisades|                1|                null|  90272|       4|   6|\n| 9479348| 598.9|    Apartment|Entire home/apt|           7|        2|Real Bed|             strict|           0| NYC|                     f|              100%|2017-07-03|   Hell's Kitchen|                1|                  60|  10036|       3|   4|\n| 8596057|420.47|        House|   Private room|           1|        2|Real Bed|           flexible|           0|  LA|                     f|              100%|2016-04-20|             null|                0|                null|  91748|       1|   1|\n|11525500|478.75|    Apartment|Entire home/apt|           2|        1|Real Bed|           flexible|           1| NYC|                     f|              100%|2017-10-07|     Williamsburg|                2|                 100|  11206|       1|   1|\n+--------+------+-------------+---------------+------------+---------+--------+-------------------+------------+----+----------------------+------------------+----------+-----------------+-----------------+--------------------+-------+--------+----+\nonly showing top 5 rows\n\nroot\n |-- id: integer (nullable = true)\n |-- price: double (nullable = true)\n |-- property_type: string (nullable = true)\n |-- room_type: string (nullable = true)\n |-- accommodates: integer (nullable = true)\n |-- bathrooms: integer (nullable = true)\n |-- bed_type: string (nullable = true)\n |-- cancellation_policy: string (nullable = true)\n |-- cleaning_fee: integer (nullable = true)\n |-- city: string (nullable = true)\n |-- host_identity_verified: string (nullable = true)\n |-- host_response_rate: string (nullable = true)\n |-- host_since: date (nullable = true)\n |-- neighbourhood: string (nullable = true)\n |-- number_of_reviews: integer (nullable = true)\n |-- review_scores_rating: integer (nullable = true)\n |-- zipcode: integer (nullable = true)\n |-- bedrooms: integer (nullable = true)\n |-- beds: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "airbnb_host_searches_df = spark.read.format('csv')\\\n",
    "                   .option('header','true')\\\n",
    "                   .option('inferschema','true')\\\n",
    "                   .load('/FileStore/tables/six_modified-1.csv')\n",
    "\n",
    "airbnb_host_searches_df.show(5)\n",
    "airbnb_host_searches_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f2af30e-9fc2-49ca-a64f-107e6f0f18dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+---------+---------+\n|host_popularity|min_price|avg_price|max_price|\n+---------------+---------+---------+---------+\n|    Trending Up|   361.09|   476.28|   685.65|\n|        Popular|   270.81|   472.81|   667.83|\n|            Hot|   340.12|   464.23|   633.51|\n|         Rising|   355.53|   503.85|   717.01|\n|            New|   313.55|   515.92|   741.76|\n+---------------+---------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "airbnb_host_searches_df.select(concat('price','room_type','host_since','zipcode','number_of_reviews')\\\n",
    "                                .alias('host_id'),'price','number_of_reviews').distinct()\\\n",
    "                                .withColumn('host_popularity',when(col('number_of_reviews') == 0,'New')\\\n",
    "                                .when((col('number_of_reviews') >= 1) & (col('number_of_reviews') <=5),'Rising')\\\n",
    "                                .when((col('number_of_reviews')>=6) & (col('number_of_reviews')<=15),'Trending Up')\\\n",
    "                                .when((col('number_of_reviews') >= 16) & (col('number_of_reviews')<=40),'Popular')\\\n",
    "                                .when((col('number_of_reviews')>40),'Hot'))\\\n",
    "                                .drop(col('host_id'),col('number_of_reviews'))\\\n",
    "                                .groupBy(col('host_popularity'))\\\n",
    "                                    .agg(round(min(col('price')),2).alias('min_price'),\\\n",
    "                                         round(avg(col('price')),2).alias('avg_price'),\\\n",
    "                                         round(max(col('price')),2).alias('max_price')).show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4212c391-bcec-43d2-8687-4f7a9d46523c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark_SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e50a069-f970-4b1e-9ee3-729fd59f2bef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "airbnb_host_searches_df.createOrReplaceTempView('airbnb_host_searches_df_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aa8cb1e-5743-48cb-bdf3-f83244d7f4a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+---------+---------+\n|host_popularity|min_price|avg_price|max_price|\n+---------------+---------+---------+---------+\n|    Trending Up|   361.09|   476.28|   685.65|\n|        Popular|   270.81|   472.81|   667.83|\n|            Hot|   340.12|   464.23|   633.51|\n|         Rising|   355.53|   503.85|   717.01|\n|            New|   313.55|   515.92|   741.76|\n+---------------+---------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select \n",
    "    case \n",
    "        when number_of_reviews = 0 then 'New' \n",
    "        when (number_of_reviews>=1 and number_of_reviews<=5) then 'Rising'\n",
    "        when (number_of_reviews>=6 and number_of_reviews<=15) then 'Trending Up'\n",
    "        when (number_of_reviews>=16 and number_of_reviews<=40) then 'Popular'\n",
    "        when number_of_reviews > 40 then 'Hot' \n",
    "    end as host_popularity , \n",
    "    round(min(price),2) as min_price,\n",
    "    round(avg(price),2) as avg_price,\n",
    "    round(max(price),2) as max_price\n",
    "from\n",
    "    (\n",
    "    select \n",
    "    distinct concat(price,room_type,host_since,zipcode,number_of_reviews) as host_id,\n",
    "    price , \n",
    "    number_of_reviews\n",
    "    from airbnb_host_searches_df_table\n",
    "    ) as U\n",
    "group by host_popularity\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8d00c42-2d2b-4be1-9537-6ce79c33680b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 7. Cookbook Recipes\n",
    "\n",
    "Q. You are given the table with titles of recipes from a cookbook and their page numbers. You are asked to represent how the recipes will be distributed in the book.\n",
    "Produce a table consisting of three columns: left_page_number, left_title and right_title. The k-th row (counting from 0), should contain the number and the title of the page with the number (2 x K) in the first and second columns respectively, and the title of the page with the number (2 x k)+1 in the third column.\n",
    "Each page contains at most 1 recipe. If the page does not contain a recipe, the appropriate cell should remain empty (NULL value). Page 0 (the internal side of the front cover) is guaranteed to be empty.     \n",
    "[Link](https://platform.stratascratch.com/coding/2089-cookbook-recipes?code_type=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39e6345d-2d36-4444-95d2-08d596b675ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark-Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c316d5c-05e2-4f35-a4b1-c433cf78787d",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n|page_number|         title|\n+-----------+--------------+\n|          1|Scrambled eggs|\n|          2|        Fondue|\n|          3|      Sandwich|\n|          4|   Tomato soup|\n|          6|         Liver|\n|         11|    Fried duck|\n|         12|   Boiled duck|\n|         15| Baked chicken|\n+-----------+--------------+\n\nroot\n |-- page_number: integer (nullable = true)\n |-- title: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "my_data = ([\n",
    "            (1\t,'Scrambled eggs'),\n",
    "            (2\t,'Fondue'),\n",
    "            (3\t,'Sandwich'),\n",
    "            (4\t,'Tomato soup'),\n",
    "            (6\t,'Liver'),\n",
    "            (11\t,'Fried duck'),\n",
    "            (12\t,'Boiled duck'),\n",
    "            (15\t,'Baked chicken')\n",
    "          ])\n",
    "\n",
    "my_schema = StructType([ StructField('page_number',IntegerType()) ,\n",
    "                        StructField('title',StringType()) ])\n",
    "\n",
    "cookbook_titles_df = spark.createDataFrame(my_data,my_schema)\n",
    "\n",
    "cookbook_titles_df.show()\n",
    "cookbook_titles_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75370d06-49b2-4a58-bb35-337eee2e0867",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+--------------+\n|left_page_number| left_title|   right_title|\n+----------------+-----------+--------------+\n|               0|       null|Scrambled eggs|\n|               2|     Fondue|      Sandwich|\n|               4|Tomato soup|          null|\n|               6|      Liver|          null|\n|               8|       null|          null|\n|              10|       null|    Fried duck|\n|              12|Boiled duck|          null|\n|              14|       null| Baked chicken|\n+----------------+-----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "cookbook_titles_df1 = cookbook_titles_df.withColumn('left_page_number', ((row_number().over(Window.orderBy('page_number')))-1)*2)\n",
    "\n",
    "cookbook_titles_df_alias = cookbook_titles_df.alias(\"df\")\n",
    "cookbook_titles_df1_alias = cookbook_titles_df1.alias(\"df1\")\n",
    "\n",
    "result  =   cookbook_titles_df1.join(cookbook_titles_df,cookbook_titles_df1_alias['left_page_number'] == \n",
    "                    cookbook_titles_df_alias['page_number'] ,\"left\")\\\n",
    "                    .withColumn('right_pgno',col('left_page_number') +1)\\\n",
    "                    .drop(cookbook_titles_df1_alias['title'],cookbook_titles_df1_alias['page_number'],'page_number')\\\n",
    "                    .withColumnRenamed(\"title\",\"left_title\")\n",
    "\n",
    "result.join(cookbook_titles_df,result[\"right_pgno\"] == cookbook_titles_df[\"page_number\"],\"left\")\\\n",
    "                                                        .drop(\"right_pgno\",\"page_number\")\\\n",
    "                                                        .withColumnRenamed(\"title\",\"right_title\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "331a68d4-2008-4edc-b416-875046f52a1e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark_SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7cbb7aa-bd07-42bc-9367-7b7887189c5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cookbook_titles_df.createOrReplaceTempView('cookbook_titles_df_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "094507c0-bed1-47bb-a255-3cb2bafd9670",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+--------------+\n|left_page_number| left_title|   right_title|\n+----------------+-----------+--------------+\n|               0|       null|Scrambled eggs|\n|               2|     Fondue|      Sandwich|\n|               4|Tomato soup|          null|\n|               6|      Liver|          null|\n|               8|       null|          null|\n|              10|       null|    Fried duck|\n|              12|Boiled duck|          null|\n|              14|       null| Baked chicken|\n+----------------+-----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select left_page_number, \n",
    "b.title as left_title ,\n",
    "c.title as right_title\n",
    "from \n",
    "    (\n",
    "        select \n",
    "        (row_number() over(ORDER BY page_number)-1)*2 as left_page_number ,\n",
    "        (row_number() over(ORDER BY page_number)-1)*2+1 as right_page_number\n",
    "        from\n",
    "        cookbook_titles_df_table\n",
    "    ) as a\n",
    "left join cookbook_titles_df_table b\n",
    "on a.left_page_number = b.page_number\n",
    "left join cookbook_titles_df_table c\n",
    "on a.right_page_number = c.page_number\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d146ac6-eada-4cb8-8ed0-5b68ed13307c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 8. Retention Rate\n",
    "\n",
    "Q. Find the monthly retention rate of users for each account separately for Dec 2020 and Jan 2021. Retention rate is the percentage of active users an account retains over a given period of time. In this case, assume the user is retained if he/she stays with the app in any future months. For example, if a user was active in Dec 2020 and has activity in any future month, consider them retained for Dec. You can assume all accounts are present in Dec 2020 and Jan 2021. Your output should have the account ID and the Jan 2021 retention rate divided by Dec 2020 retention rate.           \n",
    "[Link](https://platform.stratascratch.com/coding/2053-retention-rate?code_type=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbd530cb-af3e-418f-828c-048a4bd8de24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark-Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7189b948-84c6-4838-a5b8-7aa0977b96e5",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+\n|      date|account_id|user_id|\n+----------+----------+-------+\n|2021-01-01|        A1|     U1|\n|2021-01-01|        A1|     U2|\n|2021-01-06|        A1|     U3|\n|2021-01-02|        A1|     U1|\n|2020-12-24|        A1|     U2|\n+----------+----------+-------+\nonly showing top 5 rows\n\nroot\n |--  date: date (nullable = true)\n |-- account_id: string (nullable = true)\n |-- user_id: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    " sf_events_df = spark.read.format('csv')\\\n",
    "                        .option('header','true')\\\n",
    "                        .option('inferschema','true')\\\n",
    "                        .option('mode','PERMISSIVE')\\\n",
    "                        .load('/FileStore/tables/eight.csv')\n",
    "\n",
    "sf_events_df.show(5)\n",
    "sf_events_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5da70d8-018c-4a5c-8192-254928904552",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n|account_id|retention_rate|\n+----------+--------------+\n|        A1|           1.0|\n|        A2|           1.0|\n|        A3|           0.0|\n+----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "sf_events_df.select(date_format(col(' date'),\"yyyy-MM\").alias('date'),\"account_id\")\\\n",
    "        .withColumn('presence_in_month', lit('1'))\\\n",
    "        .distinct().sort('date','account_id')\\\n",
    "        .withColumn('dec_retension',lead('presence_in_month').over(Window.partitionBy('account_id').orderBy('date')))\\\n",
    "        .withColumn('jan_retension',lead('dec_retension').over(Window.partitionBy('account_id').orderBy('date')))\\\n",
    "        .filter(col('date') == '2020-12')\\\n",
    "        .withColumn('retention_rate',coalesce(col('jan_retension')/col('dec_retension'),lit(0)))\\\n",
    "        .select('account_id','retention_rate').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e06cad11-b534-4e8c-8522-15b51175d0f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark_SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "409e6ecd-60cf-4064-a207-3fb27a25921c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sf_events_df.createOrReplaceTempView('sf_events_df_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc686b79-b809-40ea-a315-73ec03608889",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n|account_id|retention_rate|\n+----------+--------------+\n|        A1|           1.0|\n|        A2|           1.0|\n|        A3|           0.0|\n+----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "select account_id , coalesce((jan_retension/dec_retension),0) as retention_rate\n",
    "from\n",
    "(select mnth,account_id , dec_retension,\n",
    "lead(dec_retension) over(Partition by account_id order by mnth) as jan_retension\n",
    "from \n",
    "    (select mnth , account_id , \n",
    "    lead(1) over(Partition by account_id order by mnth) as dec_retension \n",
    "    from\n",
    "        ( select date_format( `sf_events_df_table`.` date` ,\"yyyy-MM\") as mnth,\n",
    "        account_id ,\n",
    "        count(user_id) as cnt\n",
    "        from sf_events_df_table\n",
    "        group by mnth , account_id\n",
    "        order by account_id \n",
    "        ) as U\n",
    "    ) as T\n",
    "where mnth != '2021-02' \n",
    ") as V\n",
    "where mnth != '2021-01' \n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9183db17-3800-4f53-a7db-5fda1233fe43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 9. The Most Popular Client_Id Among Users Using Video and Voice Calls\n",
    "\n",
    "Q. Select the most popular client_id based on a count of the number of users who have at least 50% of their events from the following list: 'video call received', 'video call sent', 'voice call received', 'voice call sent'.   \n",
    "[Link](https://platform.stratascratch.com/coding/2029-the-most-popular-client_id-among-users-using-video-and-voice-calls?code_type=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d367a30a-0590-41b8-81da-7fb922f7a50d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark-Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05a3a5b7-172d-4905-881b-c597e5f6ebb7",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+-----------+---------+-------------------+--------+\n| id|   time_id|   user_id|customer_id|client_id|         event_type|event_id|\n+---+----------+----------+-----------+---------+-------------------+--------+\n|  1|2020-02-28|3668-QPYBK|     Sendit|  desktop|       message sent|       3|\n|  2|2020-02-28|7892-POOKP|  Connectix|   mobile|      file received|       2|\n|  3|2020-04-03|9763-GRSKD|     Zoomit|  desktop|video call received|       7|\n|  4|2020-04-02|9763-GRSKD|  Connectix|  desktop|video call received|       7|\n|  5|2020-02-06|9237-HQITU|     Sendit|  desktop|video call received|       7|\n+---+----------+----------+-----------+---------+-------------------+--------+\nonly showing top 5 rows\n\nroot\n |--  id: integer (nullable = true)\n |-- time_id: date (nullable = true)\n |-- user_id: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- client_id: string (nullable = true)\n |-- event_type: string (nullable = true)\n |-- event_id: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "fact_events_df = spark.read.format('csv')\\\n",
    "                        .option('header','true')\\\n",
    "                        .option('inferschema','true')\\\n",
    "                        .option('mode','PERMISSIVE')\\\n",
    "                        .load('/FileStore/tables/nine.csv')\n",
    "\n",
    "fact_events_df.show(5)\n",
    "fact_events_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5275aeaa-52cf-4a6b-97b7-73572f45e5cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|client_id|\n+---------+\n|  desktop|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "fact_events_df.select('client_id','user_id','event_type')\\\n",
    "        .withColumn('tot_user_events',count('event_type').over(Window.partitionBy('user_id').orderBy('user_id')))\\\n",
    "        .withColumn('favorable_event',when(((col('event_type') == 'video call received') | \n",
    "                                           (col('event_type') == 'video call sent') |\n",
    "                                           (col('event_type') == 'voice call received') |\n",
    "                                           (col('event_type') == 'voice call sent') ) , 1 ))\\\n",
    "        .drop(col('event_type')).groupBy('user_id','client_id','tot_user_events')\\\n",
    "                                .agg(count('favorable_event').alias('favorable_event'))\\\n",
    "        .withColumn('prcnt_event',col('favorable_event')/col('tot_user_events'))\\\n",
    "        .filter(col('prcnt_event')>= 0.5).select('client_id')\\\n",
    "        .groupby('client_id').agg(count('client_id').alias('cnt')).sort(col('cnt').desc()).limit(1)\\\n",
    "        .select('client_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e026f6ed-f86b-40dc-a99b-023a487595b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark_SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1760b59d-1fa1-4b19-a5ee-15920f4a5fbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fact_events_df.createOrReplaceTempView('fact_events_df_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32972f1c-5850-4bb1-9700-519ab9f21f5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|client_id|\n+---------+\n|  desktop|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "with cte as\n",
    "(select user_id \n",
    "from\n",
    "(select distinct\n",
    "user_id,\n",
    "round(((sum(special_event) over(partition by user_id) )/tot_user_events)*100,2) as prcnt_event\n",
    "from\n",
    "    (select user_id,\n",
    "    event_type,\n",
    "    count(event_type) over(partition by user_id) as tot_user_events,\n",
    "    case when event_type='video call received'or\n",
    "              event_type='video call sent'or\n",
    "              event_type='voice call received'or\n",
    "              event_type='voice call sent'\n",
    "        then  1\n",
    "    end as special_event\n",
    "    from fact_events_df_table\n",
    "    order by user_id\n",
    "    ) as U\n",
    ") as V\n",
    "where prcnt_event>=50)\n",
    "\n",
    "select client_id \n",
    "from cte a\n",
    "left join fact_events_df_table b\n",
    "on a.user_id = b.user_id\n",
    "group by client_id\n",
    "order by count(client_id) desc\n",
    "limit 1\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e4713f6-0c79-4439-8418-0501209414f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 10. Marketing Campaign Success [Advanced]\n",
    "\n",
    "Q. You have a table of in-app purchases by user. Users that make their first in-app purchase are placed in a marketing campaign where they see call-to-actions for more in-app purchases. Find the number of users that made additional in-app purchases due to the success of the marketing campaign.\n",
    "The marketing campaign doesn't start until one day after the initial in-app purchase so users that only made one or multiple purchases on the first day do not count, nor do we count users that over time purchase only the products they purchased on the first day.   \n",
    "[Link](https://platform.stratascratch.com/coding/514-marketing-campaign-success-advanced?code_type=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "104a1116-a9ea-4e42-a264-cbd5f63d92ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark-Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f44680b-8f8a-4c54-bdec-d5203906a059",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+--------+-----+\n| user_id|created_at|product_id|quantity|price|\n+--------+----------+----------+--------+-----+\n|      10|2019-01-01|       101|       3|   55|\n|      10|2019-01-02|       119|       5|   29|\n|      10|2019-03-31|       111|       2|  149|\n|      11|2019-01-02|       105|       3|  234|\n|      11|2019-03-31|       120|       3|   99|\n+--------+----------+----------+--------+-----+\nonly showing top 5 rows\n\nroot\n |--  user_id: integer (nullable = true)\n |-- created_at: date (nullable = true)\n |-- product_id: integer (nullable = true)\n |-- quantity: integer (nullable = true)\n |-- price: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "marketing_campaign_df = spark.read.format('csv')\\\n",
    "                        .option('header','true')\\\n",
    "                        .option('inferschema','true')\\\n",
    "                        .option('mode','PERMISSIVE')\\\n",
    "                        .load('/FileStore/tables/ten.csv')\n",
    "\n",
    "marketing_campaign_df.show(5)\n",
    "marketing_campaign_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b7925a4-a0aa-40be-bdbd-0ce7a2eadeb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[28]: 23"
     ]
    }
   ],
   "source": [
    "window_spec_1 = Window.partitionBy(\" user_id\").orderBy(\"created_at\")\n",
    "window_spec_2 = Window.partitionBy(\" user_id\" , \"product_id\").orderBy(\"created_at\")\n",
    "\n",
    "marketing_campaign_df.withColumn(\"date_rank\", dense_rank().over(window_spec_1)) \\\n",
    "            .withColumn(\"prod_rank\", row_number().over(window_spec_2))\\\n",
    "            .filter((col('date_rank')>1) & (col('prod_rank') == 1))\\\n",
    "            .select(' user_id').distinct().count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a614f664-a7c3-45db-adde-e19ff60a5796",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Spark_SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0caec2e0-3f92-4a8c-9f8d-085f218a2b34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "marketing_campaign_df.createOrReplaceTempView('marketing_campaign_df_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15508b38-d270-4726-b4ea-ac19e63c53e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n|count(DISTINCT  user_id)|\n+------------------------+\n|                      23|\n+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select count(distinct `U`.` user_id`)\n",
    "from          \n",
    "    (select *,\n",
    "    (dense_rank() over(partition by `marketing_campaign_df_table`.` user_id` order by created_at)) as date_rank ,\n",
    "    (dense_rank() over(partition by `marketing_campaign_df_table`.` user_id`, product_id order by created_at)) as prod_rank\n",
    "    from\n",
    "    marketing_campaign_df_table\n",
    "    ) as U\n",
    "where (date_rank>1) \n",
    "and (prod_rank==1)\n",
    "          \"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2020647449967033,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark Transformations",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
